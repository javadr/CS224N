# CS224N
My solutions for CS224n [Natural Language Processing with Deep Learning] Stanford / Winter 2021

---

## Course links

- Course page [CS224N](http://cs224n.stanford.edu/)
- Lecture videos 2021 [Youtube](https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)
- [Stanford online hub](http://onlinehub.stanford.edu/cs224)

## Links introduced by assignments

#### Assignment 1

* [Course Description and Materials](http://web.stanford.edu/class/cs224n/index.html#schedule)
* [The CS231N Python/Numpy tutorial](https://cs231n.github.io/python-numpy-tutorial/)
* [Word Embedding](https://en.wikipedia.org/wiki/Word_embedding) "*conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension*"
* You shall know a word by the company it keeps ([Firth, J. R. 1957:11](https://en.wikipedia.org/wiki/John_Rupert_Firth))
* [co-occurrence matrices](http://web.stanford.edu/class/cs124/lec/vectorsemantics.video.pdf)
* [co-occurrence matrices](https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285)
* [a slow, friendly introduction to SVD](https://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf)
* [al.pdf). 
* If you want to learn more thoroughly about PCA or SVD, feel free to check out lectures  [7](https://web.stanford.edu/class/cs168/l/l7.pdf), [8](http://theory.stanford.edu/~tim/s15/l/l8.pdf), and [9](https://web.stanford.edu/class/cs168/l/l9.pdf) of [CS168](https://web.stanford.edu/class/cs168)
* [Truncated SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition#Truncated_SVD)
* [Accessing Text Corpora and Lexical Resources](https://www.nltk.org/book/ch02.html)
* [How to flatten a list of lists](https://coderwall.com/p/rcmaea/flatten-a-list-of-lists-in-one-line-in-python)
* [Python list comprehensions](https://python-3-patterns-idioms-test.readthedocs.io/en/latest/Comprehensions.html)
* [Python sets](https://www.w3schools.com/python/python_sets.asp)
* [Python NumPy tutorial](http://cs231n.github.io/python-numpy-tutorial/)
* [sklearn.decomposition.TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)
* [matplotlib scatter plot annotate / set text at / label each point](https://web.archive.org/web/20190924160434/https://www.pythonmembers.club/2018/05/08/matplotlib-scatter-plot-annotate-set-text-at-label-each-point/)
* [matplotlib-scatter-plot-annotate-set-text-at-label-each-point/)
* [the Matplotl=ib gallery](https://matplotlib.org/gallery/index.html)
* [Computation on Arrays: Broadcasting by Jake VanderPlas](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html)
* [GloVe's original paper](https://nlp.stanford.edu/pubs/glove.pdf)
* [L1 Norm](http://mathworld.wolfram.com/L1-Norm.html) 
* [L2 Norm](http://mathworld.wolfram.com/L2-Norm.html)
* [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)
* [Polysemy](https://en.wikipedia.org/wiki/Polysemy)
* [GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar)
* [The word analogy testing caveat](https://www.aclweb.org/anthology/N18-2039.pdf)

